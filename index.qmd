---
title: "Predictive Capabilities of LASSO Regression"
author: "Kayla Hayes, HJ Kim, Kristy Tarano, Andrew Melara-Suazo"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

[Presentation](slides.html)

## Introduction & Literature Review

Genetics continue to shape human existence, even after thousands of years. With the use of advancing technology and increasingly available genotype and phenotype data, biomedical scientists are making significant strides in the understanding of genetic predisposition later leading to the development of diseases or disorders [@claussnitzer2020]. The time commitment required for decision making in the diagnosis of cases involving genome variation is daunting due to the complex nature of the clinical effects, and as a consequence the medical industry now relies more heavily on technology to assist in these diagnoses. In one article, a collaboration between China's BGI Genomics and Fuyang People's Hospital demonstrated the abilities of a chip designed to detect the many variations in genes. The end goal of chips like these is to lower overall time required and cost of diagnoses, especially those relating to genetic mutations [@liu2023]. <br>

Even with technology, however, there are still many inefficiencies to address. One of these inefficiencies exists in the overuse of Computerized Clinical Decision Support Systems (CCDSS) to order lab tests such as in the case of Fondazione Policlinico Universitario Agostino Gemelli, a large university hospital located in Rome. This study found that labs are required to make critical judgments in 70 to 80% of cases, causing it to become common practice for medical professionals to order many tests that are unnecessary [@tamburrano2020]. Studies like that of the CCDSS in the Italian university hospital put the research community at a crossroads because these computerized diagnosis systems are intended to be an assistive tool for medical professionals to utilize. Crossroads aside, researchers continue to break boundaries in the diagnoses of genetic disorders with the increasing application of machine learning to create predictive models. Because so many diseases and disorders are linked to varying degrees of gene expression and mutation, proper organization of these categories through classification methods is needed to gain insight [@raza2022]. <br>

Linear models are proven to be an ample method of predicting disorder outcomes through targeting root causes and gene expressions, and analyzing the subsequent patterns that are extrapolated from them [@liu2019]. Classification techniques such as logistic regression are capable of predicting various outcome variables related to cancer status and the probability of demographics causing an event like this to occur [@meysami2023]. Optimizing logistic regression models with a regularization method known as LASSO regression creates a simplified overall model and an increase in selected independent variables for a predictive outcome [@rusyana2021]. LASSO, or Least Absolute Shrinkage and Selection Operator regression is a robust technique used to address common modeling issues like overfitting, overestimation, and multicollinearity [@ranstam2018]. LASSO regression's power as a regularization method is exhibited best when applied to datasets demonstrating the aforementioned problems, such as the multicollinearity that naturally exists in vehicle crash data [@abdulhafedh2022]. <br>

The Mean Squared Error is commonly used to measure the accuracy of any given regression model and when utilizing LASSO regression, any coefficients with very high values can be reduced all the way down to zero by the introduction of a bias [@hodson2022]. LASSO adds a penalization parameter, also referred to as the L1 Penalty, which determines how much shrinkage will occur to the model's coefficients [@freijeiro2022]. The L1 Penalty is determined by Î» multiplied against the absolute value of the slope of a fitted line where the sum of squared residuals is added to the penalty to increase the bias and offset the variance of the model which proves to be useful for developing predictive models [@greenwood2020]. The use of LASSO regression to perform feature selection produces a higher level of accuracy and precision when coupled with a machine learning model to predict gene patterns relating to cancer diagnosis [@guha2024]. <br>

In this analysis, demographic factors and health indicators are used to predict various disorder sub-classes. Moreover, the optimizing capabilities of LASSO regression are combined with the predictive capabilities of multinomial logistic regression to train a model that can determine a correct disorder and subclass outcome. Several participant identifying fields are omitted from the analysis and the primary goal is to solely use key demographic and health information to make all determinations. This model is ideally assists in better understanding what factors play a role in the diagnoses of specific disorders as well as how machine learning can be used as a tool to increase the efficiency of medical diagnoses.

## Methods

LASSO regression was conducted as a means to tune our model and to
minimize variables with low impact to our outcome variable. To develop a
baseline for performing LASSO Regression, we began with a multinomial
logistic regression model. The multinomial logistic regression model is
formulated as shown below:

$$ logit(Pi)=ln(\frac{Pi}{Preference})  = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k $$

In this model, the logarithm of the odds is set equal to the intercept
value plus the coefficients of X. The assumption of this function is
that p(X) should always be between 0 and 1. This logistic regression
formula consequentially forms into an S shaped curve to fit those
parameters.

$$ \hat{\beta} = \arg\min_{\beta} \left( \frac{1}{2n} \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right) $$

LASSO regression is an optimizing feature selection technique used to
assist with common regression problems such as the aforementioned
reduction of overfitting through regularization. The purpose of using a
regularization technique such as LASSO regression is to introduce a
penalization parameter known as L1. L1 is determined by multiplying
Lambda with the sums of the absolute values of the estimated parameters, demonstrated by the portion of the formula shown here:

$$ \lambda \sum_{j=1}^p |\beta_j| $$


K-Fold Cross Validation involves splitting the data into "k" number of folds. The first formula shown here computes log loss for each fold which then is summed up to get an overall error measure for each lambda. K-Fold Cross Validation was used to determine the best lambda value, that results in the minimum mean cross-validated error (log loss) [@yates2023].

$$ \text{Log Loss} = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] $$

$$ \text{K-Fold Cross Validation} = \frac{1}{K} \sum_{k=1}^K \text{Err}_k $$

## Analysis and Results

### Data and Visualization

The data employed throughout this study is sourced from a genomics dataset which recorded various demographic and health indicators
describing a set of patients' respective disorders and disorder subclasses. The dataset contains 45 variables that are a mix between
categorical and continuous types which ultimately describe the genetic disorders. This data in its original form is split into individual training and testing datasets, but for this study only the training dataset is utilized. The training data itself is split into its own training and testing set for proof of concept.

#### Variables in Original Dataset

```{r, warning=FALSE, echo=T, message=FALSE}

# Packages Used
 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(dplyr)
library(DT)
library(glmnet)
library(fastDummies)
library(nnet)
library(plyr)
library(readxl)
#install.packages("naniar")
library(naniar) 
#install.packages("VIM")
library(data.table)
#install.packages("mltools")
library(mltools)
library(ggplot2)
library(reshape2)
library(tidyr)
library(purrr)

# Load Data

Variables <- read.csv("Variables - Sheet1.csv", header = TRUE)

# Display the Variables table

datatable(Variables)

```

#### Target Data from Original Dataset

```{r, warning=FALSE, echo=TRUE}

Target <- read.csv("Target - Sheet1.csv", header = TRUE)

# Display the Target Data table

datatable(Target)

```

#### Data Cleaning

During pre-processing, non-informative patient identification variables
were cleaned from the dataset to remove insignificant predictive
variables relating to the outcome. A multinomial logistic regression
model was then developed to predict a genetic disorder outcome for
patients based solely on their recorded information.

Because missing values were minimal, omission of missing values was
performed to reduce the bias of introducing an imputation method as well
as to meet the assumptions of binary logistic regression. The dataset
was split using a 20:80 ratio, and K-Fold Cross Validation was used to
find the most appropriate lambda value for performing LASSO regression.
A lambda value of (0.0002451) was yielded and a multinomial logistic
regression was then performed using R. Cross validation also reported a
deviance of 99.9%. This means the goodness-of-fit is very high and that
LASSO regression is able to account for virtually all variability
amongst the variables of importance. The final model had an accuracy of
0.494 which was confirmed with a confusion matrix. The confusion matrix
is able to create a ratio of the total amount of predictions that were
true. This ultimately confirms the performance of the model on the
testing data and predicting genetic disorder outcomes.

## Statistical Analysis

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(glmnet)
library(fastDummies)
library(nnet)

#Loading data 
genetic_data <- read_csv('train_genetics.csv')

#Removing NA from data
clean_gene_data <- na.omit(genetic_data)

#Creating dummy columns
clean_gene_data <- dummy_cols(clean_gene_data, select_columns = c("Respiratory Rate (breaths/min)","Gender" ,"Heart Rate (rates/min", "H/O radiation exposure (x-ray)", "Birth asphyxia", "H/O substance abuse", "Birth defects", "H/O substance abuse", "Blood test result", "Disorder Subclass"), remove_first_dummy = TRUE)

# for consistency
set.seed(234)

# Get row indices for the training set and setting split
train_indices <- sample(seq_len(nrow(clean_gene_data)), size = 0.8 * nrow(clean_gene_data))

# Split the data into 80:20
train_data <- clean_gene_data[train_indices, ] # should be about 80% of the original data
test_data <- clean_gene_data[-train_indices, ] # should be about 20% of the original data


#Defining the response variable
y <- as.factor(train_data$`Genetic Disorder`)


#Defining the matrix of predictor variables for the model
x <- data.matrix(train_data[,c ("Genes in mother's side",'Maternal gene','Paternal gene' ,'Inherited from father', 'Blood cell count (mcL)', 'Respiratory Rate (breaths/min)_Tachypnea', 'Heart Rate (rates/min_Tachycardia','Gender_Female', 'Gender_Male','Birth asphyxia_No record', 'Birth asphyxia_Not available','Birth asphyxia_Yes','Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)_No', 'H/O radiation exposure (x-ray)_Not applicable', 'H/O radiation exposure (x-ray)_Yes', 'H/O substance abuse_No', 'H/O substance abuse_Not applicable', 'H/O substance abuse_Yes', 'Assisted conception IVF/ART', 'History of a0malies in previous pregnancies', 'Birth defects_Singular', 'Blood test result_inconclusive', 'Blood test result_normal', 'Blood test result_slightly abnormal','White Blood cell count (thousand per microliter)', 'Symptom 1', 'Symptom 2','Symptom 3','Symptom 4', 'Symptom 5', 'Test 1', 'Test 2','Test 3','Test 4', 'Test 5', 'Disorder Subclass_Cancer', 'Disorder Subclass_Cystic fibrosis', 'Disorder Subclass_Diabetes', 'Disorder Subclass_Hemochromatosis', "Disorder Subclass_Leber's hereditary optic neuropathy", 'Disorder Subclass_Leigh syndrome', 'Disorder Subclass_Mitochondrial myopathy', 'Disorder Subclass_Tay-Sachs')])


#Looking for optimal lamda value using k-fold cross validation
cross_val_model <- cv.glmnet(x, y, family = "multinomial", alpha= 1)

#Looking for the best lambda value 
min_lambda <- cross_val_model$lambda.min

#Value of Lambda
min_lambda

#Graph of test MSE error
plot(cross_val_model)

#Creating our LASSO regression model 
gene_model <- glmnet(x, y, family = "multinomial", alpha = 1, lambda = min_lambda)

#Coefficients of the model
coef(gene_model)
print(gene_model)

#Creating matrix of predictor variables from test data
x_test <- data.matrix(test_data[,c ("Genes in mother's side",'Maternal gene','Paternal gene' ,'Inherited from father', 'Blood cell count (mcL)', 'Respiratory Rate (breaths/min)_Tachypnea', 'Heart Rate (rates/min_Tachycardia','Gender_Female', 'Gender_Male','Birth asphyxia_No record', 'Birth asphyxia_Not available','Birth asphyxia_Yes','Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)_No', 'H/O radiation exposure (x-ray)_Not applicable', 'H/O radiation exposure (x-ray)_Yes', 'H/O substance abuse_No', 'H/O substance abuse_Not applicable', 'H/O substance abuse_Yes', 'Assisted conception IVF/ART', 'History of a0malies in previous pregnancies', 'Birth defects_Singular', 'Blood test result_inconclusive', 'Blood test result_normal', 'Blood test result_slightly abnormal','White Blood cell count (thousand per microliter)', 'Symptom 1', 'Symptom 2','Symptom 3','Symptom 4', 'Symptom 5', 'Test 1', 'Test 2','Test 3','Test 4', 'Test 5', 'Disorder Subclass_Cancer', 'Disorder Subclass_Cystic fibrosis', 'Disorder Subclass_Diabetes', 'Disorder Subclass_Hemochromatosis', "Disorder Subclass_Leber's hereditary optic neuropathy", 'Disorder Subclass_Leigh syndrome', 'Disorder Subclass_Mitochondrial myopathy', 'Disorder Subclass_Tay-Sachs')])

#Defining response variable from test data 
y_test <- as.factor(test_data$`Genetic Disorder`)



#CONFUSION MATRIX BELOW 

#Creating prediction classes for the model
pred_prob <- predict(gene_model, newx = x_test, s = min_lambda, type = "class")

#Creating the prediction probability threshold for the model
pred_class <- ifelse(pred_prob > 0.05, 1, 0)

#Creating the confusion matrix
confusion_matrix <- table(Predicted = pred_class, Actual = y_test)

print(confusion_matrix)

#Calculates the overall true positive/negatives over the sum of all predictions
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

# Print the accuracy decimal
print(paste("Accuracy:", round(accuracy, 4)))

#Decimal proportion of true positives over true and false positive predictions
precision <- diag(confusion_matrix) / rowSums(confusion_matrix)

#Decimal proportion of true positives over true and false negative predictions
recall <- diag(confusion_matrix) / colSums(confusion_matrix)

#F1 Score measures accuracy and considers precision and recall and also accounts for false + and -
f1_score <- 2 * (precision * recall) / (precision + recall)
 

f1_score


```

## Conclusion

After applying a multinomial logistic regression model to the data,
LASSO regression was employed to predict genetic disorder outcomes on a
genomics dataset. The dataset consisted of continuous and categorical
patient demographic and health indicator data. Pre-processing consisted
of omitting missing values and additional feature selection was
appropriately handled through LASSO regression. The LASSO multinomial
logistic regression model produced an accuracy of 0.494 supported by a
confusion matrix. This suggests that about half of the predictions were
correct in regard to the three types of possible genetic disorder
outcomes. The accuracy of the model is also contingent on the reported
Kappa value. This final data suggests that although a fair accuracy rate
exists, the model should be carefully revisited and tuned accordingly.

## References

::: {#refs}
:::
