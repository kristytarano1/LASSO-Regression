---
title: "LASSO Regression"
author: "Kayla Hayes, HJ Kim, Kristy Tarano, Andrew Melara-Suazo"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is LASSO Regression?

LASSO, or Least Absolute Shrinkage and Selection Operator, regression is a popular linear regression technique used to address common modeling issues like overfitting,  overestimation, and multicollinearity (Ranstam & Cook, 2018). Datasets containing a large number of features can lead to complex models, and this can affect the reliability of the model outcomes. LASSO regression’s power as a regularization method is exhibited best when applied to datasets demonstrating these problems (Abdulhafedh, A., 2022). The Mean Squared Error (MSE) is commonly used to measure the accuracy of any given regression model. When utilizing LASSO regression, any coefficients with very high values can be reduced all the way down to zero by the introduction of a bias. LASSO adds a penalization parameter, also referred to as the L1 Penalty, which determines how much shrinkage will occur to the model’s coefficients (Freijeiro-González, Febrero-Bande, & González-Manteiga, 2022). The L1 Penalty is determined by  λ  times the slope of a fitted line where the sum of squared residuals is added to the penalty to increase the bias and offset the variance of the model. This leads to a lower MSE as a result and allows for the benefits of feature selection. By shrinking some of the coefficients, through penalization, it thus filters which variables will be considered and included in the final model (Fonti & Belitser, 2017). 

This is my work and I want to add more work...

### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

The dataset chosen for this study relates to Genomes and Genetics. It was originally used in a Machine Learning contest for contestants to design models with the ability to predict genetic disorders and their related disorder subclasses. The dataset consists of two parts: training data and test data. It is broken down into 45 variables that are applied across both the training data and the testing data. The tables below show the breakdown of the variables and the target data.


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(dplyr)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data

Target <- read.csv("Target - Sheet1.csv", header = TRUE)
Variables <- read.csv("Variables - Sheet1.csv", header = TRUE)

# Print the first few rows of the data frame to verify it loaded correctly


kable(head(Target))

kable(head(Variables))

# ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

#  ggplot1 + geom_point(aes(col=region), size = 4) +
#  geom_text_repel(aes(label=abb)) +
#  scale_x_log10() +
#  scale_y_log10() +
#  geom_smooth(formula = "y~x", method=lm,se = F)+
#  xlab("Populations in millions (log10 scale)") + 
#  ylab("Total number of murders (log10 scale)") +
#  ggtitle("US Gun Murders in 2010") +
#  scale_color_discrete(name = "Region")+
#      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
