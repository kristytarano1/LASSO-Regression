---
title: "LASSO Regression"
author: "Kayla Hayes, HJ Kim, Kristy Tarano, Andrew Melara-Suazo"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is LASSO Regression?

  Genetic disorders and the diagnoses of problematic and infrequent disorders have shaped our existence for thousands of years [@claussnitzer2020].  The time commitment to diagnose each case correctly and efficiently is a daunting one to say the least [@liu2023]. Medical instruments are expensive and the inefficiencies that exist with overuse of computerized decision making forecasting is apparent in labs and hospitals, in this case a large university hospital [@tamburrano2020]. Studies such as the aforementioned put the research community at a crossroads since these computerized diagnosing systems are intended to be a great asset and tool for medical professionals to utilize during their diagnosing procedures. With that being said, machine learning and the diagnosing of genetic disorders has made great strides in the genomics world with an increasing variety of machine learning models being applied to predictive modeling [@raza2022]. Linear models have shown to be an ample way of predicting disorder outcomes through the targeting of what is often the root cause, gene expression and the patterns that can be extrapolated  [@liu2019]. Aside from the work of linear models, many of these disorders have been linked to varying degrees of gene expression and mutation, proper organizing of these categories through means such as a classification method provides insight  [@raza2022]. Moreover, the classification technique, Logistic regression, is capable of predicting various outcome variables related to cancer status and the probability of such an event occurring due to demographic data [@meysami2023]. Choosing to optimize a technique like Logistic regression with a regularization method such as LASSO regression indicates a simplified overall model and an increase in selected independent variables for a predictive outcome[@rusyana2021]. <br>
  
  LASSO, or Least Absolute Shrinkage and Selection Operator regression is a robust technique used to address common modeling issues like overfitting, overestimation, and multicollinearity [@ranstam2018]. LASSO regression’s power as a regularization method is exhibited best when applied to datasets demonstrating these problems such as this study on multicollinearity in vehicle crash data [@abdulhafedh2022].  The Mean Squared Error is commonly used to measure the accuracy of any given regression model and when utilizing LASSO regression, any coefficients with very high values can be reduced all the way down to zero by the introduction of a bias [@hodson2022].  LASSO adds a penalization parameter, also referred to as the L1 Penalty, which determines how much shrinkage will occur to the model’s coefficients [@freijeiro2022].  The L1 Penalty is determined by λ multiplied by the absolute value of the slope of a fitted line where the sum of squared residuals is added to the penalty to increase the bias and offset the variance of the model which has shown to be useful for developing predictive models [@greenwood2020]. The use of LASSO regression to perform feature selection has shown to be productive when coupled with a machine learning model to reach a high level of accuracy and precision when predicting gene patterns relating to cancer diagnosis [@guha2024]. <br>
  
  In this study, we propose to use demographic factors and health indicators to predict various disorder sub-classes. Moreover, we combine the optimizing capabilities of LASSO regression with the predictive capabilities of multinomial logistic regression to train our model so that we can determine a correct disorder and subclass outcome. We state that several participant identifying fields are omitted from the analysis and that the primary goal is to solely use key demographic and health information to make all determinations. This model is being developed to better understand what factors play a role in the diagnoses of specific disorders as well as how machine learning can be used as a tool to increase the efficiency of medical diagnoses.

## Methods

LASSO regression was conducted as a means to tune our model and to minimize variables with low impact to our outcome variable. To develop a baseline for performing LASSO Regression, we began with a binary logistic regression model: 


...Add new formula image here (figure out image files in quarto)


In this model, the logarithm of the odds is set equal to the intercept value plus the coefficients of X. The assumption of this function is that p(X) should always be between 0 and 1. This logistic regression formula consequentially forms into an S shaped curve to fit those parameters.   

LASSO regression is an optimizing feature selection technique used to assist regression techniques such as the aforementioned reduction of overfitting through regularization.   


...Add new formula image here (figure out image files in quarto)


The purpose of using a regularization technique such as LASSO regression is to introduce a  penalization parameter known as  L1. L1 is determined by multiplying Lambda with the sums of the absolute values of the estimated parameters. 


...Add new formula image here (figure out image files in quarto)


K-Fold Cross Validation involves splitting the data into (n) number of folds. L(f^​−i​,Di​) computes log loss which is summed with the errors for (n) number of folds. k-Fold cross validation also can interpret what variables account for variability in the model.  

## Analysis and Results

### Data and Visualization

The data employed throughout this study was sourced from a genomics dataset which recorded various demographic and health indicators describing a set of patients’ respective disorders and disorder subclasses. The dataset contains 45 variables that are a mix between categorical and continuous types which ultimately describe the genetic disorders. This data was originally split into individual training and testing datasets, but for this study only the training dataset was utilized. The training data was split into its own training and testing set for proof of concept. 

During pre-processing, non-informative patient identification variables were cleaned from the dataset to remove insignificant predictive variables relating to the outcome. A multinomial logistic regression model was then developed to predict a genetic disorder outcome for patients based solely on their recorded information.  

Because missing values were minimal, omission of missing values was performed to reduce the bias of introducing an imputation method as well as to meet the assumptions of logistic regression. The dataset was split using a 20:80 ratio, and K-fold cross validation was used to find the most appropriate lambda value for performing LASSO regression. A lambda value of (0.0002451) was yielded and a multinomial logistic regression was then performed using R. Cross validation also reported a deviance of  99.9%. This means the goodness-of-fit is very high and that LASSO regression is able to account for virtually all variability in the model with our variables of importance. The final model had an accuracy of 0.494 which was confirmed with a confusion matrix. The confusion matrix is able to create a ratio of the total amount of predictions that were true. This ultimately confirms the performance of the model on the testing data and predicting genetic disorder outcomes. 



```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(dplyr)
library(DT)
```

### Variables

```{r, warning=FALSE, echo=TRUE}
# Load Data

Variables <- read.csv("Variables - Sheet1.csv", header = TRUE)

# Display the Variables table

datatable(Variables)

```

### Target Data

```{r, warning=FALSE, echo=TRUE}

Target <- read.csv("Target - Sheet1.csv", header = TRUE)

# Display the Target Data table

datatable(Target)

```

### Exploratory Data Analysis

```{r, warning=FALSE, echo=TRUE}

library(tidyverse)
library(plyr)
library(readxl)
#install.packages("naniar")
library(naniar) 
#install.packages("VIM")
library(data.table)
#install.packages("mltools")
library(mltools)
library(ggplot2)
library(reshape2)
library(tidyr)
library(purrr)
#getwd()
#setwd("E:/UWF/STA6257")

df <- read_excel("train_genetics.xlsx")
dim(df)
glimpse(df)
df <- df[, !names(df) %in% c("Patient Id", "Patient First Name", "Family Name", "Father's name", "Institute Name", "Location of Institute", "Test 1", "Test 2", "Test 3", "Test 4", "Test 5", "Symptom 1", "Symptom 2", "Symptom 3", "Symptom 4", "Symptom 5", "Parental consent", "Follow-up", "H/O radiation exposure (x-ray)", "H/O substance abuse", "Birth asphyxia")]
glimpse(df)

# Missing value check

naniar::miss_var_summary(df)

VIM::aggr(df,prop=FALSE,numbers=TRUE)

#df <- df %>% filter(!is.na(df$`Mother's age`))
#df <- df %>% filter(!is.na(df$`Father's age`))
#VIM::aggr(df,prop=FALSE,numbers=TRUE)
#naniar::miss_var_summary(df)

dt <- na.omit(df)
dim(dt)
sum(is.na(dt))

is.data.table(dt) # to see if data.table
dt <- as.data.table(dt)

# Convert Categorical data into numerical data

# Function to convert categorical variables to numeric
convert_to_numeric <- function(x) {
  if (is.numeric(x)) return(x)
  factor_x <- as.factor(x)
  as.numeric(factor_x)
}

# List of columns to convert (excluding already numeric columns and ID)

columns_to_convert <- c("Status", "Respiratory Rate (breaths/min)", "Heart Rate (rates/min", "Autopsy shows birth defect (if applicable)", "Place of birth", "Assisted conception IVF/ART", "History of anomalies in previous pregnancies","Birth defects", "Blood test result", "Genetic Disorder", "Disorder Subclass", "Gender")

# Convert specified columns to numeric
dt[, (columns_to_convert) := lapply(.SD, convert_to_numeric), .SDcols = columns_to_convert]

# Special handling for "Respiratory Rate" and "Heart Rate" columns
dt[, `Respiratory Rate (breaths/min)` := as.numeric(sub("Normal \\(30-60\\)", "Tachycardia", `Respiratory Rate (breaths/min)`))]
dt[, `Heart Rate (rates/min` := as.numeric(sub("Normal", "Tachycardia", `Heart Rate (rates/min`))]
dt[, `Gender` := as.numeric(sub("Male", "Female", `Gender`))]


# Convert Yes/No columns to 1/0
yes_no_columns <- c("Genes in mother's side", "Inherited from father", "Maternal gene", "Paternal gene", "Folic acid details (peri-conceptional)", "H/O serious maternal illness")

dt[, (yes_no_columns) := lapply(.SD, function(x) as.numeric(x == "Yes")), .SDcols = yes_no_columns]


VIM::aggr(dt,prop=FALSE,numbers=TRUE)

```

### Converted Data Table

```{r, warning=FALSE, echo=TRUE}

datatable(dt)

```


### Histograms of Converted Data

```{r, warning=FALSE, echo=TRUE}

# EDA
# graphs

dt %>%
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

```


### Correlation Heatmap

```{r, warning=FALSE, echo=TRUE}

# creating correlation matrix
corr_mat <- round(cor(dt),2)
melted_corr_mat <- melt(corr_mat)

# plotting the correlation heatmap
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,
                                   fill=value)) + 
  geom_tile()


```


### Statistical Analysis - Experiencing Errors for Definition of X (see code)

```{r, warning=FALSE, echo=TRUE}

# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(glmnet)

genetic_data <- read.csv("train_genetics.csv", header = TRUE)

#genetic_data <- read_csv('/Users/kristytarano/Desktop/Stats for Data Science II/train_genetics.csv')

clean_gene_data <- na.omit(genetic_data)

#Defining the response variable
#y <- clean_gene_data$`Genetic Disorder`

y <- clean_gene_data$`Disorder Subclass`

#Defining the matrix of predictor variables for the model
#x <- data.matrix(clean_gene_data[,c ("Genes in mother's side", 'Inherited from father', 'Maternal gene', 'Paternal gene', 'Blood cell count (mcL)', 'Respiratory Rate (breaths/min)', 'Heart Rate (rates/min', 'Test 1', 'Test 2', 'Test 3', 'Test 4', 'Test 5', 'Gender', 'Birth asphyxia', 'Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)', 'H/O substance abuse', 'Assisted conception IVF/ART', 'History of a0malies in previous pregnancies', 'Birth defects', 'White Blood cell count (thousand per microliter)', 'Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5')])


                 
#Looking for optimal lambda value using k-fold cross validation
#cross_val_model <- cv.glmnet(x, y, alpha= 1)

#Looking for the best lambda value to produce lowest MSE
#min_lambda <- cross_val_model$lambda.min

#min_lambda


#Graph of test MSE error
#plot(cross_val_model)


#Creating our LASSO regression model 

#gene_model <- glmnet(x, y, alpha = 1, lambda = min_lambda)


#coef(gene_model)


#y_prediction <- predict(gene_model, s=min_lambda, newx = x)

#finding SST and SSE
#sst <- sum((y - mean(y))^2)
#sse <- sum ((y_prediction - y)^2)

#rsq <- 1 - sse/sst

#rsq


#test_data<- read_csv('/Users/kristytarano/Desktop/Stats for Data Science II/test.csv')

#clean_test_data <- na.omit(test_data)

#clean2_test_data <-as_tibble((clean_test_data %>% 
                   # dplyr::select(`Genes in mother's side`,`Maternal gene` ,`Paternal gene` ,`Inherited from father`, `Blood cell count (mcL)`, `Respiratory Rate (breaths/min)`, `Heart Rate (rates/min`,`Gender`,`Birth asphyxia`, `Folic acid details (peri-conceptional)`, `H/O serious maternal illness`, `H/O radiation exposure (x-ray)`, `H/O substance abuse`, `Assisted conception IVF/ART`, `History of a0malies in previous pregnancies`, `Birth defects`, `White Blood cell count (thousand per microliter)`, `Symptom 1`, `Symptom 2`,`Symptom 3`,`Symptom 4`, `Symptom 5`, `Test 1`, `Test 2`,`Test 3`,`Test 4`, `Test 5`)))


```

## Conclusion

After applying a multinomial logistic regression model to the data, LASSO regression was employed to predict genetic disorder outcomes on a genomics dataset. The dataset consisted of continuous and categorical patient demographic and health indicator data. Pre-processing consisted of omitting missing values and additional feature selection was appropriately handled through LASSO regression. The LASSO multinomial logistic regression model produced an accuracy of 0.494 supported by a confusion matrix. This suggests that about half of the predictions were correct in regard to the three types of possible genetic disorder outcomes. The accuracy of the model is also contingent on the reported Kappa value. This final data suggests that although a fair accuracy rate exists, the model should be carefully revisited and tuned accordingly. 

## References

<div id="refs"></div>



