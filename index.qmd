---
title: "LASSO Regression"
author: "Kayla Hayes, HJ Kim, Kristy Tarano, Andrew Melara-Suazo"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is LASSO Regression?

  Genetic disorders and the diagnoses of problematic and infrequent disorders have shaped our existence for thousands of years [@claussnitzer2020].  The time commitment to diagnose each case correctly and efficiently is a daunting one to say the least [@liu2023]. Medical instruments are expensive and the inefficiencies that exist with overuse of computerized decision making forecasting is apparent in labs and hospitals, in this case a large university hospital [@tamburrano2020]. Studies such as the aforementioned put the research community at a crossroads since these computerized diagnosing systems are intended to be a great asset and tool for medical professionals to utilize during their diagnosing procedures. With that being said, machine learning and the diagnosing of genetic disorders has made great strides in the genomics world with an increasing variety of machine learning models being applied to predictive modeling [@raza2022]. Linear models have shown to be an ample way of predicting disorder outcomes through the targeting of what is often the root cause, gene expression and the patterns that can be extrapolated  [@liu2019]. Aside from the work of linear models, many of these disorders have been linked to varying degrees of gene expression and mutation, proper organizing of these categories through means such as a classification method provides insight  [@raza2022]. Moreover, the classification technique, Logistic regression, is capable of predicting various outcome variables related to cancer status and the probability of such an event occurring due to demographic data [@meysami2023]. Choosing to optimize a technique like Logistic regression with a regularization method such as LASSO regression indicates a simplified overall model and an increase in selected independent variables for a predictive outcome[@rusyana2021]. <br>
  
  LASSO, or Least Absolute Shrinkage and Selection Operator regression is a robust technique used to address common modeling issues like overfitting, overestimation, and multicollinearity [@ranstam2018]. LASSO regression’s power as a regularization method is exhibited best when applied to datasets demonstrating these problems such as this study on multicollinearity in vehicle crash data [@abdulhafedh2022].  The Mean Squared Error is commonly used to measure the accuracy of any given regression model and when utilizing LASSO regression, any coefficients with very high values can be reduced all the way down to zero by the introduction of a bias [@hodson2022].  LASSO adds a penalization parameter, also referred to as the L1 Penalty, which determines how much shrinkage will occur to the model’s coefficients [@freijeiro2022].  The L1 Penalty is determined by λ multiplied by the absolute value of the slope of a fitted line where the sum of squared residuals is added to the penalty to increase the bias and offset the variance of the model which has shown to be useful for developing predictive models [@greenwood2020]. The use of LASSO regression to perform feature selection has shown to be productive when coupled with a machine learning model to reach a high level of accuracy and precision when predicting gene patterns relating to cancer diagnosis [@guha2024]. <br>
  
  In this study, we propose to use demographic factors and health indicators to predict various disorder sub-classes. Moreover, we combine the optimizing capabilities of LASSO regression with the predictive capabilities of multinomial logistic regression to train our model so that we can determine a correct disorder and subclass outcome. We state that several participant identifying fields are omitted from the analysis and that the primary goal is to solely use key demographic and health information to make all determinations. This model is being developed to better understand what factors play a role in the diagnoses of specific disorders as well as how machine learning can be used as a tool to increase the efficiency of medical diagnoses.

### Related work

This section is going to cover the literature review...

## Methods

The data used was sourced from a genomics dataset that recorded various demographic and health indicators related to patients and their respective disorder and subclasses. The dataset contains within it 45 variables pertaining to the genetic disorders. Several variables were cleaned from the dataset to remove non informative variables that would not be predictive of an outcome variable. A model was developed to predict a disorder and subclass outcome for patients based solely on their recorded information. LASSO regression was used as a means to tune our model and to minimize variables with low impact to our outcome variable. To develop a baseline for performing LASSO Regression, we began with a traditional Linear Regression model:

$$
y_t = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + \varepsilon_i
$$

In this model, Y_t represents the dependent variable that is determined by k number of independent variables beginning with X1, X2, and so on. B0 represents the intercept value, or the value of Y when all values of X are equal to zero, while Bi represents the coefficients of X. The coefficients (betas), along with the mean and standard deviation of the residuals (epsilons), constitute the parameters of the model [@nau2014]. The resulting equation with this frame of reference in mind is demonstrated below: 

$$
y(hat)_t = \beta_0 + \beta_1 X_1t + \beta_2 X_2t + ... + \beta_k X_kt
$$

The assumptions of the simple Linear Regression model are as follows [@bumc_correlation_regression]:

  Linearity: The relationship between the dependent variables Xi and the independent variable Y must be linear, which means any change in X is proportional to a change in Y. 
  Homoscedasticity: The variance of the residuals across each value of the dependent variables Xi should remain constant.
  Independence: The errors demonstrated for one observation should not predict the error of any other observation. 
  Normality: For all values of the dependent variable Xi, the data should be normally distributed. 
  
  
With a simple Linear Regression model established, LASSO Regression was then applied to the split data to shrink coefficient magnitudes of dependent variables Xi that have low relevance. We used LASSO Regression to utilize its feature selection capabilities which introduced the L1 Penalty (lambda) as shown in the model below. This aims to shrink a number of the coefficients that were not addressed in the initial data preparation Bj to 0. The priority afterwards was to address which dependent variables had the most significant coefficients which allows the model to make the most accurate outcome variable prediction on a testing dataset [@ogutu2012].

LASSO model equation will go here...

## Analysis and Results

### Data and Visualization

The dataset chosen for this study relates to Genomes and Genetics. It was originally used in a Machine Learning contest for contestants to design models with the ability to predict genetic disorders and their related disorder subclasses. The dataset consists of two parts: training data and test data. It is broken down into 45 variables that are applied across both the training data and the testing data. The tables below show the breakdown of the variables and the target data.


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(dplyr)
library(DT)
```

### Variables

```{r, warning=FALSE, echo=TRUE}
# Load Data

Variables <- read.csv("Variables - Sheet1.csv", header = TRUE)

# Display the Variables table

datatable(Variables)

```

### Target Data

```{r, warning=FALSE, echo=TRUE}

Target <- read.csv("Target - Sheet1.csv", header = TRUE)

# Display the Target Data table

datatable(Target)

```

### Exploratory Data Analysis

```{r, warning=FALSE, echo=TRUE}

library(tidyverse)
library(plyr)
library(readxl)
#install.packages("naniar")
library(naniar) 
#install.packages("VIM")
library(data.table)
#install.packages("mltools")
library(mltools)
library(ggplot2)
library(reshape2)
library(tidyr)
library(purrr)
#getwd()
#setwd("E:/UWF/STA6257")

df <- read_excel("train_genetics.xlsx")
dim(df)
glimpse(df)
df <- df[, !names(df) %in% c("Patient Id", "Patient First Name", "Family Name", "Father's name", "Institute Name", "Location of Institute", "Test 1", "Test 2", "Test 3", "Test 4", "Test 5", "Symptom 1", "Symptom 2", "Symptom 3", "Symptom 4", "Symptom 5", "Parental consent", "Follow-up", "H/O radiation exposure (x-ray)", "H/O substance abuse", "Birth asphyxia")]
glimpse(df)

# Missing value check

naniar::miss_var_summary(df)

VIM::aggr(df,prop=FALSE,numbers=TRUE)

#df <- df %>% filter(!is.na(df$`Mother's age`))
#df <- df %>% filter(!is.na(df$`Father's age`))
#VIM::aggr(df,prop=FALSE,numbers=TRUE)
#naniar::miss_var_summary(df)

dt <- na.omit(df)
dim(dt)
sum(is.na(dt))

is.data.table(dt) # to see if data.table
dt <- as.data.table(dt)

# Convert Categorical data into numerical data

# Function to convert categorical variables to numeric
convert_to_numeric <- function(x) {
  if (is.numeric(x)) return(x)
  factor_x <- as.factor(x)
  as.numeric(factor_x)
}

# List of columns to convert (excluding already numeric columns and ID)

columns_to_convert <- c("Status", "Respiratory Rate (breaths/min)", "Heart Rate (rates/min", "Autopsy shows birth defect (if applicable)", "Place of birth", "Assisted conception IVF/ART", "History of anomalies in previous pregnancies","Birth defects", "Blood test result", "Genetic Disorder", "Disorder Subclass", "Gender")

# Convert specified columns to numeric
dt[, (columns_to_convert) := lapply(.SD, convert_to_numeric), .SDcols = columns_to_convert]

# Special handling for "Respiratory Rate" and "Heart Rate" columns
dt[, `Respiratory Rate (breaths/min)` := as.numeric(sub("Normal \\(30-60\\)", "Tachycardia", `Respiratory Rate (breaths/min)`))]
dt[, `Heart Rate (rates/min` := as.numeric(sub("Normal", "Tachycardia", `Heart Rate (rates/min`))]
dt[, `Gender` := as.numeric(sub("Male", "Female", `Gender`))]


# Convert Yes/No columns to 1/0
yes_no_columns <- c("Genes in mother's side", "Inherited from father", "Maternal gene", "Paternal gene", "Folic acid details (peri-conceptional)", "H/O serious maternal illness")

dt[, (yes_no_columns) := lapply(.SD, function(x) as.numeric(x == "Yes")), .SDcols = yes_no_columns]


VIM::aggr(dt,prop=FALSE,numbers=TRUE)

```

### Converted Data Table

```{r, warning=FALSE, echo=TRUE}

datatable(dt)

```


### Histograms of Converted Data

```{r, warning=FALSE, echo=TRUE}

# EDA
# graphs

dt %>%
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

```


### Correlation Heatmap

```{r, warning=FALSE, echo=TRUE}

# creating correlation matrix
corr_mat <- round(cor(dt),2)
melted_corr_mat <- melt(corr_mat)

# plotting the correlation heatmap
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,
                                   fill=value)) + 
  geom_tile()


```


### Statistical Analysis - Experiencing Errors (see code)

```{r, warning=FALSE, echo=TRUE}

# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(glmnet)

genetic_data <- read.csv("train_genetics.csv", header = TRUE)

#genetic_data <- read_csv('/Users/kristytarano/Desktop/Stats for Data Science II/train_genetics.csv')

clean_gene_data <- na.omit(genetic_data)

#Defining the response variable
#y <- clean_gene_data$`Genetic Disorder`

y <- clean_gene_data$`Disorder Subclass`

#Defining the matrix of predictor variables for the model
#x <- data.matrix(clean_gene_data[,c ("Genes in mother's side", 'Inherited from father', 'Maternal gene', 'Paternal gene', 'Blood cell count (mcL)', 'Respiratory Rate (breaths/min)', 'Heart Rate (rates/min', 'Test 1', 'Test 2', 'Test 3', 'Test 4', 'Test 5', 'Gender', 'Birth asphyxia', 'Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)', 'H/O substance abuse', 'Assisted conception IVF/ART', 'History of a0malies in previous pregnancies', 'Birth defects', 'White Blood cell count (thousand per microliter)', 'Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5')])


                 
#Looking for optimal lambda value using k-fold cross validation
#cross_val_model <- cv.glmnet(x, y, alpha= 1)

#Looking for the best lambda value to produce lowest MSE
#min_lambda <- cross_val_model$lambda.min

#min_lambda


#Graph of test MSE error
#plot(cross_val_model)


#Creating our LASSO regression model 

#gene_model <- glmnet(x, y, alpha = 1, lambda = min_lambda)


#coef(gene_model)


#y_prediction <- predict(gene_model, s=min_lambda, newx = x)

#finding SST and SSE
#sst <- sum((y - mean(y))^2)
#sse <- sum ((y_prediction - y)^2)

#rsq <- 1 - sse/sst

#rsq


#test_data<- read_csv('/Users/kristytarano/Desktop/Stats for Data Science II/test.csv')

#clean_test_data <- na.omit(test_data)

#clean2_test_data <-as_tibble((clean_test_data %>% 
                   # dplyr::select(`Genes in mother's side`,`Maternal gene` ,`Paternal gene` ,`Inherited from father`, `Blood cell count (mcL)`, `Respiratory Rate (breaths/min)`, `Heart Rate (rates/min`,`Gender`,`Birth asphyxia`, `Folic acid details (peri-conceptional)`, `H/O serious maternal illness`, `H/O radiation exposure (x-ray)`, `H/O substance abuse`, `Assisted conception IVF/ART`, `History of a0malies in previous pregnancies`, `Birth defects`, `White Blood cell count (thousand per microliter)`, `Symptom 1`, `Symptom 2`,`Symptom 3`,`Symptom 4`, `Symptom 5`, `Test 1`, `Test 2`,`Test 3`,`Test 4`, `Test 5`)))


```

## Conclusion

## References

<div id="refs"></div>



